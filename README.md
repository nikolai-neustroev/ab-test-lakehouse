# ab-test-lakehouse
Data Engineering Zoomcamp 2025 - Course Project

The ab-test-lakehouse project is a data engineering pipeline designed as part of the Data Engineering Zoomcamp 2025 course. It leverages Google Cloud Platform (GCP) services to process and analyze A/B testing data. The pipeline includes infrastructure provisioning with Terraform, data ingestion via Pub/Sub, data processing with Dataproc and PySpark, and data storage in BigQuery for further analysis and visualization.

### Data

I use mock data in this project. The data imitates user funnel events of an online store during the A/B testing: homepage visit, product view, placing to cart, making an order, writing a review. Events are splitted by users, A/B tests and test groups. [More here](https://github.com/nikolai-neustroev/ab-test-lakehouse-mock-data).

### Architecture

This project implements the data lakehouse architecture - the data is stored in Google Cloud Storage as Apache Iceberg tables. In data lakehouse a transactional storage software layer runs on top of existing data lake and makes it work more like a relational database. A small amount of data is stored in relational data warehouse for the purpose of demonstration, but it is possible to store all data as Iceberg tables.

The event stream, a continuous flow of real-time data generated by systems, applications, or devices, is used as a data source. Each event represents a single occurrence or change in state, often formatted as a message or record, and is immutable and timestamped.

Dataflow and Dataproc used to ingest and process data. BigQuery allows us to explore data with SQL. Looker Studio is used as a visualization tool.

### Key Components

| **Component** | **Screenshot** |
|---|:---:|
| **Terraform** is used to provision GCP resources in IaC fashion |  |
| **Pub/Sub** is used for data streaming | ![](https://private-user-images.githubusercontent.com/38642966/431856604-8495ee27-ebbf-4a1d-bc46-2f557aaad80e.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NDQyMDQ1OTcsIm5iZiI6MTc0NDIwNDI5NywicGF0aCI6Ii8zODY0Mjk2Ni80MzE4NTY2MDQtODQ5NWVlMjctZWJiZi00YTFkLWJjNDYtMmY1NTdhYWFkODBlLnBuZz9YLUFtei1BbGdvcml0aG09QVdTNC1ITUFDLVNIQTI1NiZYLUFtei1DcmVkZW50aWFsPUFLSUFWQ09EWUxTQTUzUFFLNFpBJTJGMjAyNTA0MDklMkZ1cy1lYXN0LTElMkZzMyUyRmF3czRfcmVxdWVzdCZYLUFtei1EYXRlPTIwMjUwNDA5VDEzMTEzN1omWC1BbXotRXhwaXJlcz0zMDAmWC1BbXotU2lnbmF0dXJlPWViODg3OWVjMmUzMDdkNTRhMWJjMjJlOThiN2I5MjkyOTkxNTU0ZTg0M2ExNDBjYzU1MjVmYTdjZmFkNmE2YzAmWC1BbXotU2lnbmVkSGVhZGVycz1ob3N0In0.wRKmzfm7Cg27kpZo7oQnUMThGZNj0OAoUZ7xvjpG-E8 "Pub/Sub") |
| **Dataflow** is used to ingest data from Pub/Sub and store in GCP bucket | f |
| **Apache Iceberg** is used as project's data lake. Data is partioned by experiment_uuid which ensures the uniform storage of the data | f |
| **Hive Metastore** is configured for Iceberg table management | f |
| **PySpark** scripts process data from CSV and JSON files, converting them into Iceberg tables | f |
| A workflow template orchestrates the execution of PySpark jobs on **Dataproc** | f |
| Processed data is stored in **BigQuery** and made accessible via an external Iceberg table and a native table | f |
| Results are visualized using **Looker Studio** | f |

### Prerequisites
1. gcloud (Google Cloud SDK 509.0.0)
2. terraform (Terraform v1.11.2)

### Steps to reproduce
1. Start a new GCP project. 
2. Copy `terraform.tfvars.example` file and rename it to `terraform.tfvars`. Put your project ID into it instead of "my-unique-project-id".
3. Run `terraform apply`.
4. While the cloud infrastructure is emerging, generate mock data in ab-test-lakehouse-mock-data repo.
5. After Pub/Sub successfully created, send mock data review_written messages to it.
6. Upload remaining files to the dataproc_bucket, `csvs` folder.
7. In dataproc-bucket create `binaries` folder and upload [Iceberg binary](https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.8.1/iceberg-spark-runtime-3.5_2.12-1.8.1.jar) file.
8. In dataproc-bucket create `scripts` folder and upload *.py files from `pyspark` local folder.
9. In ./pyspark copy `dataproc.env.example` file to `dataproc.env` and set your variables.
10. Run `create_dataproc_workflow.sh`.
11. Go to [BigQuery](https://console.cloud.google.com/bigquery) and create a dataset named `dataset`.
12. Go to [Dataproc](https://console.cloud.google.com/dataproc/workflows/templates) and run the `pyspark-funnel-ab-template`.
13. After completion return to BigQuery and run `external_table.sql` query. Change the URI if required.
14. Go to [Looker Studio](https://lookerstudio.google.com/) and use tables from `dataset` as sources.

### Next steps

1. Implement orchestration with Airflow or similar tool.
2. Add data quality tests.
