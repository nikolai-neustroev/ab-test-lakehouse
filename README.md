# ab-test-lakehouse
Data Engineering Zoomcamp 2025 - Course Project

The ab-test-lakehouse project is a data engineering pipeline designed as part of the Data Engineering Zoomcamp 2025 course. It leverages Google Cloud Platform (GCP) services to process and analyze A/B testing data. The pipeline includes infrastructure provisioning with Terraform, data ingestion via Pub/Sub, data processing with Dataproc and PySpark, and data storage in BigQuery for further analysis and visualization.

### Data

I use mock data in this project. The data imitates user funnel events of an online store during the A/B testing: homepage visit, product view, placing to cart, making an order, writing a review. Events are splitted by users, A/B tests and test groups. [More here](https://github.com/nikolai-neustroev/ab-test-lakehouse-mock-data).

### Architecture

![Image](https://github.com/user-attachments/assets/2d4054a4-1feb-45af-8515-caae30544580 "Architecture")

This project implements the data lakehouse architecture - the data is stored in Google Cloud Storage as Apache Iceberg tables. In data lakehouse a transactional storage software layer runs on top of existing data lake and makes it work more like a relational database. A small amount of data is stored in relational data warehouse for the purpose of demonstration, but it is possible to store all data as Iceberg tables.

The event stream, a continuous flow of real-time data generated by systems, applications, or devices, is used as a data source. Each event represents a single occurrence or change in state, often formatted as a message or record, and is immutable and timestamped.

Dataflow and Dataproc used to ingest and process data. BigQuery allows us to explore data with SQL. Looker Studio is used as a visualization tool.

### Key Components

| **Component** | **Screenshot** |
|---|:---:|
| **Terraform** is used to provision GCP resources in IaC fashion | ![](https://github.com/user-attachments/assets/e325f5b4-182f-4b48-9ce2-c263a4a3cefb "Terraform") |
| **Pub/Sub** is used for data streaming | ![](https://github.com/user-attachments/assets/8495ee27-ebbf-4a1d-bc46-2f557aaad80e "Pub/Sub") |
| **Dataflow** is used to ingest data from Pub/Sub and store in GCP bucket | ![](https://github.com/user-attachments/assets/e2bf1fa0-2644-4ddd-ba15-5e0107edfd29 "Dataflow") |
| **Apache Iceberg** is used as project's data lake. Data is partioned by experiment_uuid which ensures the uniform storage of the data | ![](https://github.com/user-attachments/assets/36e5109f-b8f6-473b-8327-1212daf91f88 "Iceberg") |
| **Hive Metastore** is configured for Iceberg table management | ![](https://github.com/user-attachments/assets/b1060461-fa65-44d8-bf6a-2546e2c0fff6 "Metastore") |
| **PySpark** scripts process data from CSV and JSON files, converting them into Iceberg tables | ![](https://github.com/user-attachments/assets/a2d68ea6-75b8-4d50-ba4c-afd7144fd000 "PySpark") |
| A workflow template orchestrates the execution of PySpark jobs on **Dataproc** | ![](https://github.com/user-attachments/assets/30ae41bd-ae38-43b2-8217-884d8ec7ef34 "Dataproc") |
| Processed data is stored in **BigQuery** and made accessible via an external Iceberg table and a native table | ![](https://github.com/user-attachments/assets/67cabf23-751a-4ea7-8c08-b1882f1bb214 "BigQuery") |
| Results are visualized using **Looker Studio** | ![](https://github.com/user-attachments/assets/a77a9d5f-7c73-4f8e-804d-d0addc5d1d2e "Looker Studio") |

### Prerequisites
1. gcloud (Google Cloud SDK 509.0.0)
2. terraform (Terraform v1.11.2)

### Steps to reproduce
1. Start a new GCP project. 
2. Copy `terraform.tfvars.example` file and rename it to `terraform.tfvars`. Put your project ID into it instead of "my-unique-project-id".
3. Run `terraform apply`.
4. While the cloud infrastructure is emerging, generate mock data in ab-test-lakehouse-mock-data repo.
5. After Pub/Sub successfully created, send mock data review_written messages to it.
6. Upload remaining files to the dataproc_bucket, `csvs` folder.
7. In dataproc-bucket create `binaries` folder and upload [Iceberg binary](https://search.maven.org/remotecontent?filepath=org/apache/iceberg/iceberg-spark-runtime-3.5_2.12/1.8.1/iceberg-spark-runtime-3.5_2.12-1.8.1.jar) file.
8. In dataproc-bucket create `scripts` folder and upload *.py files from `pyspark` local folder.
9. In ./pyspark copy `dataproc.env.example` file to `dataproc.env` and set your variables.
10. Run `create_dataproc_workflow.sh`.
11. Go to [BigQuery](https://console.cloud.google.com/bigquery) and create a dataset named `dataset`.
12. Go to [Dataproc](https://console.cloud.google.com/dataproc/workflows/templates) and run the `pyspark-funnel-ab-template`.
13. After completion return to BigQuery and run `external_table.sql` query. Change the URI if required.
14. Go to [Looker Studio](https://lookerstudio.google.com/) and use tables from `dataset` as sources.

### Next steps

1. Implement orchestration with Airflow or similar tool.
2. Add data quality tests.
